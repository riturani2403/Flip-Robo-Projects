{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on \n",
    "   www.flipkart.com and scrape following details for all the search results displayed on 1st page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(2)\n",
    "#locating the search bar\n",
    "search_bar=driver.find_element_by_class_name(\"_3704LK\")\n",
    "search_bar.send_keys('smartphone')\n",
    "\n",
    "time.sleep(2)\n",
    "#locating the button and clicking it toh search for sunglasses\n",
    "button=driver.find_element_by_class_name('L0Z3Pu')\n",
    "button.click()\n",
    "\n",
    "\n",
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "\n",
    "time.sleep(3)\n",
    "#scrapping the required details\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):#for loop for scrapping 4 page\n",
    "    brands=driver.find_elements_by_class_name('_3OO5Xc')\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "    desc=driver.find_elements_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[2]/div/div/div')\n",
    "    for i in desc:\n",
    "        description.append(i.text)#appending the description in list\n",
    "    prices=driver.find_elements_by_xpath(\"//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[2]/div/div[4]/div[1]/div/div[1]\")# scraping the price from the xpath\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "    disc=driver.find_elements_by_xpath(\"//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[2]/div/div[4]/div[1]/div/div[3]\")# scraping the discount from the xpath\n",
    "    for i in disc:\n",
    "        discount.append(i.text)\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/div\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "\n",
    "time.sleep(3)        \n",
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':brand[:100],\n",
    "                'Description':description[:100],\n",
    "                'Price':price[:100],\n",
    "                'Discount':discount[:100]})\n",
    "#printing dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write a python program to access the search bar and search button on images.google.com and \n",
    "   scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fruits\n",
    "\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "driver.get('https://www.google.com/')\n",
    "search = driver.find_element_by_name('q')\n",
    "search.send_keys('fruits',Keys.ENTER)\n",
    "\n",
    "elem = driver.find_element_by_link_text('Images')\n",
    "elem.get_attribute('href')\n",
    "elem.click()\n",
    "value = 0\n",
    "for i in range(10):\n",
    "    driver.execute_script('scrollBy(\"+ str(value) +\",+500);')\n",
    "    value += 100\n",
    "    time.sleep(4)\n",
    "elements = driver.find_elements_by_xpath('//img[contains(@class,\"rg_i\")]')\n",
    "count = 0\n",
    "for i in elements:\n",
    "    src = i.get_attribute('src')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cars\n",
    "\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "driver.get('https://www.google.com/')\n",
    "search = driver.find_element_by_name('q')\n",
    "search.send_keys('cars',Keys.ENTER)\n",
    "\n",
    "elem = driver.find_element_by_link_text('Images')\n",
    "elem.get_attribute('href')\n",
    "elem.click()\n",
    "value = 0\n",
    "for i in range(10):\n",
    "    driver.execute_script('scrollBy(\"+ str(value) +\",+500);')\n",
    "    value += 100\n",
    "    time.sleep(4)\n",
    "elements = driver.find_elements_by_xpath('//img[contains(@class,\"rg_i\")]')\n",
    "count = 0\n",
    "for i in elements:\n",
    "    src = i.get_attribute('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Machine learning\n",
    "\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "driver.get('https://www.google.com/')\n",
    "search = driver.find_element_by_name('q')\n",
    "search.send_keys('Machine Learning',Keys.ENTER)\n",
    "\n",
    "elem = driver.find_element_by_link_text('Images')\n",
    "elem.get_attribute('href')\n",
    "elem.click()\n",
    "value = 0\n",
    "for i in range(10):\n",
    "    driver.execute_script('scrollBy(\"+ str(value) +\",+500);')\n",
    "    value += 100\n",
    "    time.sleep(4)\n",
    "elements = driver.find_elements_by_xpath('//img[contains(@class,\"rg_i\")]')\n",
    "count = 0\n",
    "for i in elements:\n",
    "    src = i.get_attribute('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Write a program to scrap all the available details of top 10 gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.digit.in/search/?keyword=gaming%20laptop\"\n",
    "page = requests.get(url)\n",
    "#see content in page\n",
    "soup = BeautifulSoup(page.content)\n",
    "product_name = []  #empty list\n",
    "website=[]\n",
    "img_url = [] \n",
    "\n",
    "for i in soup11.find_all(\"div\",class_=\"searchProduct-desc\"):\n",
    "    product_name.append(i.text)\n",
    "    \n",
    "for i in soup11.find_all(\"div\",class_=\"Block-status\"):\n",
    "    website.append(i.text)\n",
    "    \n",
    "for i in soup11.find_all(\"img\",class_=\"searchProduct-ickon\"):\n",
    "    img_url.append(i.get(\"src\"))    \n",
    "    \n",
    "\n",
    "laptop=pd.DataFrame({})\n",
    "laptop['product_name']=product_name[:10]\n",
    "laptop['official_website']=website[:10]\n",
    "laptop['img_url'] = img_url[:10]\n",
    "laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on \n",
    "   google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# assign url in the webdriver object\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.google.com/maps\")\n",
    "sleep(2)\n",
    "\n",
    "\n",
    "# search locations\n",
    "def searchplace():\n",
    "    Place = driver.find_element_by_class_name(\"tactile-searchbox-input\")\n",
    "    Place.send_keys(\"Delhi\")\n",
    "    Submit = driver.find_element_by_xpath(\"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/div[1]/button\")\n",
    "    Submit.click()\n",
    "\n",
    "searchplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
